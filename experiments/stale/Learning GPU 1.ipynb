{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "Using data [Data and variances]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karenleung/repos/trams/agent_models/agent_model_utils.py:5: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-560d3d8d7975>\", line 1, in <module>\n",
      "    get_ipython().magic('matplotlib inline')\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2146, in magic\n",
      "    return self.run_line_magic(magic_name, magic_arg_s)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2067, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-107>\", line 2, in matplotlib\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/magics/pylab.py\", line 99, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2930, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/pylabtools.py\", line 307, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/matplotlib/pyplot.py\", line 229, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/matplotlib/__init__.py\", line 1305, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  mpl.use(\"QT5Agg\")\n",
      "/home/karenleung/repos/trams/util/rasterize.py:4: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-560d3d8d7975>\", line 1, in <module>\n",
      "    get_ipython().magic('matplotlib inline')\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2146, in magic\n",
      "    return self.run_line_magic(magic_name, magic_arg_s)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2067, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-107>\", line 2, in matplotlib\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/magics/pylab.py\", line 99, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2930, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/IPython/core/pylabtools.py\", line 307, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/matplotlib/pyplot.py\", line 229, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/matplotlib/__init__.py\", line 1305, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('agg')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from importlib import reload\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from jdconfig import Config\n",
    "\n",
    "from logger import *\n",
    "from tsne import *\n",
    "\n",
    "import learning\n",
    "reload(learning)\n",
    "from learning import *\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GPU = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# settings and configuration parameters\n",
    "\n",
    "rnn_params = {'dim' : 16, 'nlayers' : 1, 'proj_dim': 8}\n",
    "model_params = {'input_dim': 28, 'output_dim': 2, 'latent_dim': 5, 'fc_dim': 16, 'log_sigma_clamp': 3, 'standard_p': True, 'gmm_components': 1, 'y_max': 120, 'log_py_clamp': 0.2}\n",
    "learning_params = {'batch_size': 128, 'seed': 1, 'max_steps': 100, 'lr': 0.0001, 'kl_anneal': (0.0001, 1.0, 50, 2)}\n",
    "settings_params = {'cuda': True, 'model_dir':'training/GPU'+str(GPU), 'data_dir': '../data/npz/lanechange_slim.npz', 'GPU': GPU, 'scale': 5}\n",
    "hyperparameters = {'model': model_params, 'learning': learning_params, 'rnn': rnn_params, 'settings': settings_params}\n",
    "\n",
    "\n",
    "with open(settings_params['model_dir'] + '/hyperparameters.json', 'w') as fp:\n",
    "    json.dump(hyperparameters, fp)\n",
    "params = Config(settings_params['model_dir'] + '/hyperparameters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_np = np.load(params.settings.data_dir)\n",
    "# col_idx = np.where(data_np['col']==1)[0]\n",
    "# col_data = data_np['data_np'][col_idx,:,:]\n",
    "# col_tl = data_np['tl'][col_idx]\n",
    "\n",
    "# N = np.arange(len(col_tl))\n",
    "# np.random.shuffle(N)\n",
    "# train_ratio = .9\n",
    "# train_eval_idx = int(np.floor(train_ratio*len(col_tl)))\n",
    "# train_idx = N[:train_eval_idx]\n",
    "# eval_idx = N[train_eval_idx:]\n",
    "\n",
    "# # train data\n",
    "# train_data = torch.tensor(col_data[train_idx,:,:], dtype=torch.float)\n",
    "# train_tl = torch.tensor(col_tl[train_idx], dtype=torch.int)\n",
    "\n",
    "# # eval data\n",
    "# eval_data = torch.tensor(col_data[eval_idx,:,:], dtype=torch.float)\n",
    "# eval_tl = torch.tensor(col_tl[eval_idx], dtype=torch.int)\n",
    "\n",
    "# kwargs = {'num_workers': 1, 'pin_memory': True} if params.settings.cuda else {}\n",
    "# kwargs\n",
    "\n",
    "# data_mean, data_std = get_mean_var_pytorch(train_data, train_tl, 'slim')\n",
    "# train_loader = torch.utils.data.DataLoader(LaneswapDatasetCol(train_data, train_tl, (data_mean, data_std), scale=params.settings.scale), batch_size = params.learning.batch_size, shuffle=True, **kwargs)\n",
    "# eval_loader = torch.utils.data.DataLoader(LaneswapDatasetCol(eval_data, eval_tl, (data_mean, data_std), scale=params.settings.scale), batch_size = params.learning.batch_size, shuffle=True, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_np = np.load(params.settings.data_dir)\n",
    "N = np.arange(len(data_np['tl']))\n",
    "np.random.shuffle(N)\n",
    "train_ratio = .9\n",
    "train_eval_idx = int(np.floor(train_ratio*len(data_np['tl'])))\n",
    "train_idx = N[:train_eval_idx]\n",
    "eval_idx = N[train_eval_idx:]\n",
    "\n",
    "# train data\n",
    "train_data = torch.tensor(data_np['data_np'][train_idx,:,:], dtype=torch.float)\n",
    "train_tl = torch.tensor(data_np['tl'][train_idx], dtype=torch.int)\n",
    "train_col = torch.tensor(data_np['col'][train_idx], dtype=torch.int)\n",
    "\n",
    "# eval data\n",
    "eval_data = torch.tensor(data_np['data_np'][eval_idx,:,:], dtype=torch.float)\n",
    "eval_tl = torch.tensor(data_np['tl'][eval_idx], dtype=torch.int)\n",
    "eval_col = torch.tensor(data_np['col'][eval_idx], dtype=torch.int)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if params.settings.cuda else {}\n",
    "kwargs\n",
    "\n",
    "data_mean, data_std = get_mean_var_pytorch(train_data, train_tl, 'slim')\n",
    "train_loader = torch.utils.data.DataLoader(LaneswapDataset(train_data, train_tl, train_col, (data_mean, data_std), scale=params.settings.scale), batch_size = params.learning.batch_size, shuffle=True, **kwargs)\n",
    "eval_loader = torch.utils.data.DataLoader(LaneswapDataset(eval_data, eval_tl, eval_col, (data_mean, data_std), scale=params.settings.scale), batch_size = params.learning.batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training/GPU1/checkpoints'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.settings.model_dir + '/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THIS WILL DELETE THE FOLDERS AND CHECKPOINTS INSIDE.\n",
    "shutil.rmtree(params.settings.model_dir + '/checkpoints', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karenleung/miniconda3/envs/trams/lib/python3.5/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:  0  ---- TRAIN LOSS :  0.728001 ---- EVAL LOSS 0.725922\n",
      "STEP:  1  ---- TRAIN LOSS :  0.721566 ---- EVAL LOSS 0.720149\n",
      "STEP:  2  ---- TRAIN LOSS :  0.71717 ---- EVAL LOSS 0.718424\n",
      "STEP:  3  ---- TRAIN LOSS :  0.712507 ---- EVAL LOSS 0.714337\n",
      "STEP:  4  ---- TRAIN LOSS :  0.709551 ---- EVAL LOSS 0.711646\n",
      "STEP:  5  ---- TRAIN LOSS :  0.707082 ---- EVAL LOSS 0.709239\n",
      "STEP:  6  ---- TRAIN LOSS :  0.704203 ---- EVAL LOSS 0.705611\n",
      "STEP:  7  ---- TRAIN LOSS :  0.701006 ---- EVAL LOSS 0.704387\n",
      "STEP:  8  ---- TRAIN LOSS :  0.699269 ---- EVAL LOSS 0.705284\n",
      "STEP:  9  ---- TRAIN LOSS :  0.696931 ---- EVAL LOSS 0.698958\n",
      "STEP:  10  ---- TRAIN LOSS :  0.694915 ---- EVAL LOSS 0.700124\n",
      "STEP:  11  ---- TRAIN LOSS :  0.693003 ---- EVAL LOSS 0.694593\n",
      "STEP:  12  ---- TRAIN LOSS :  0.691856 ---- EVAL LOSS 0.693322\n",
      "STEP:  13  ---- TRAIN LOSS :  0.69047 ---- EVAL LOSS 0.698847\n",
      "STEP:  14  ---- TRAIN LOSS :  0.689152 ---- EVAL LOSS 0.695039\n",
      "STEP:  15  ---- TRAIN LOSS :  0.687723 ---- EVAL LOSS 0.692409\n",
      "STEP:  16  ---- TRAIN LOSS :  0.686449 ---- EVAL LOSS 0.690291\n",
      "STEP:  17  ---- TRAIN LOSS :  0.686559 ---- EVAL LOSS 0.686634\n",
      "STEP:  18  ---- TRAIN LOSS :  0.685132 ---- EVAL LOSS 0.69778\n",
      "STEP:  19  ---- TRAIN LOSS :  0.683807 ---- EVAL LOSS 0.690187\n",
      "STEP:  20  ---- TRAIN LOSS :  0.682124 ---- EVAL LOSS 0.689264\n",
      "STEP:  21  ---- TRAIN LOSS :  0.681424 ---- EVAL LOSS 0.687405\n",
      "STEP:  22  ---- TRAIN LOSS :  0.679568 ---- EVAL LOSS 0.687698\n",
      "STEP:  23  ---- TRAIN LOSS :  0.679639 ---- EVAL LOSS 0.689106\n",
      "STEP:  24  ---- TRAIN LOSS :  0.679933 ---- EVAL LOSS 0.68868\n",
      "STEP:  25  ---- TRAIN LOSS :  0.677892 ---- EVAL LOSS 0.685856\n",
      "STEP:  26  ---- TRAIN LOSS :  0.676481 ---- EVAL LOSS 0.684022\n",
      "STEP:  27  ---- TRAIN LOSS :  0.675709 ---- EVAL LOSS 0.684221\n",
      "STEP:  28  ---- TRAIN LOSS :  0.674454 ---- EVAL LOSS 0.681646\n",
      "STEP:  29  ---- TRAIN LOSS :  0.673267 ---- EVAL LOSS 0.688791\n",
      "STEP:  30  ---- TRAIN LOSS :  0.673512 ---- EVAL LOSS 0.683155\n",
      "STEP:  31  ---- TRAIN LOSS :  0.671685 ---- EVAL LOSS 0.683502\n",
      "STEP:  32  ---- TRAIN LOSS :  0.670891 ---- EVAL LOSS 0.676768\n",
      "STEP:  33  ---- TRAIN LOSS :  0.670106 ---- EVAL LOSS 0.686371\n",
      "STEP:  34  ---- TRAIN LOSS :  0.667946 ---- EVAL LOSS 0.680443\n",
      "STEP:  35  ---- TRAIN LOSS :  0.667831 ---- EVAL LOSS 0.686057\n",
      "STEP:  36  ---- TRAIN LOSS :  0.66864 ---- EVAL LOSS 0.682206\n",
      "STEP:  37  ---- TRAIN LOSS :  0.666317 ---- EVAL LOSS 0.681989\n",
      "STEP:  38  ---- TRAIN LOSS :  0.665938 ---- EVAL LOSS 0.682708\n",
      "STEP:  39  ---- TRAIN LOSS :  0.663773 ---- EVAL LOSS 0.690065\n",
      "STEP:  40  ---- TRAIN LOSS :  0.662874 ---- EVAL LOSS 0.684118\n",
      "STEP:  41  ---- TRAIN LOSS :  0.662218 ---- EVAL LOSS 0.682413\n",
      "STEP:  42  ---- TRAIN LOSS :  0.661769 ---- EVAL LOSS 0.680947\n",
      "STEP:  43  ---- TRAIN LOSS :  0.661326 ---- EVAL LOSS 0.684576\n",
      "STEP:  44  ---- TRAIN LOSS :  0.66194 ---- EVAL LOSS 0.677288\n",
      "STEP:  45  ---- TRAIN LOSS :  0.661434 ---- EVAL LOSS 0.689131\n",
      "STEP:  46  ---- TRAIN LOSS :  0.660518 ---- EVAL LOSS 0.682589\n",
      "STEP:  47  ---- TRAIN LOSS :  0.660848 ---- EVAL LOSS 0.673475\n",
      "STEP:  48  ---- TRAIN LOSS :  0.658064 ---- EVAL LOSS 0.685148\n",
      "STEP:  49  ---- TRAIN LOSS :  0.658132 ---- EVAL LOSS 0.673786\n",
      "STEP:  50  ---- TRAIN LOSS :  0.658792 ---- EVAL LOSS 0.690469\n",
      "STEP:  51  ---- TRAIN LOSS :  0.659253 ---- EVAL LOSS 0.673515\n",
      "STEP:  52  ---- TRAIN LOSS :  0.660717 ---- EVAL LOSS 0.681863\n",
      "STEP:  53  ---- TRAIN LOSS :  0.659718 ---- EVAL LOSS 0.681339\n",
      "STEP:  54  ---- TRAIN LOSS :  0.659242 ---- EVAL LOSS 0.68712\n",
      "STEP:  55  ---- TRAIN LOSS :  0.65943 ---- EVAL LOSS 0.678692\n",
      "STEP:  56  ---- TRAIN LOSS :  0.658991 ---- EVAL LOSS 0.691017\n",
      "STEP:  57  ---- TRAIN LOSS :  0.660056 ---- EVAL LOSS 0.681171\n",
      "STEP:  58  ---- TRAIN LOSS :  0.658666 ---- EVAL LOSS 0.678178\n",
      "STEP:  59  ---- TRAIN LOSS :  0.659391 ---- EVAL LOSS 0.674661\n",
      "STEP:  60  ---- TRAIN LOSS :  0.659583 ---- EVAL LOSS 0.675795\n",
      "STEP:  61  ---- TRAIN LOSS :  0.660788 ---- EVAL LOSS 0.680813\n",
      "STEP:  62  ---- TRAIN LOSS :  0.660722 ---- EVAL LOSS 0.690493\n",
      "STEP:  63  ---- TRAIN LOSS :  0.659086 ---- EVAL LOSS 0.678413\n",
      "STEP:  64  ---- TRAIN LOSS :  0.658438 ---- EVAL LOSS 0.681266\n",
      "STEP:  65  ---- TRAIN LOSS :  0.660429 ---- EVAL LOSS 0.685076\n",
      "STEP:  66  ---- TRAIN LOSS :  0.659968 ---- EVAL LOSS 0.682166\n",
      "STEP:  67  ---- TRAIN LOSS :  0.660101 ---- EVAL LOSS 0.675321\n",
      "STEP:  68  ---- TRAIN LOSS :  0.659202 ---- EVAL LOSS 0.67457\n",
      "STEP:  69  ---- TRAIN LOSS :  0.660759 ---- EVAL LOSS 0.678912\n",
      "STEP:  70  ---- TRAIN LOSS :  0.660404 ---- EVAL LOSS 0.688807\n",
      "STEP:  71  ---- TRAIN LOSS :  0.658421 ---- EVAL LOSS 0.692825\n",
      "STEP:  72  ---- TRAIN LOSS :  0.659912 ---- EVAL LOSS 0.685228\n",
      "STEP:  73  ---- TRAIN LOSS :  0.661251 ---- EVAL LOSS 0.682241\n",
      "STEP:  74  ---- TRAIN LOSS :  0.66052 ---- EVAL LOSS 0.68242\n",
      "STEP:  75  ---- TRAIN LOSS :  0.661555 ---- EVAL LOSS 0.684386\n",
      "STEP:  76  ---- TRAIN LOSS :  0.661448 ---- EVAL LOSS 0.687983\n",
      "STEP:  77  ---- TRAIN LOSS :  0.659076 ---- EVAL LOSS 0.689848\n",
      "STEP:  78  ---- TRAIN LOSS :  0.660658 ---- EVAL LOSS 0.676553\n",
      "STEP:  79  ---- TRAIN LOSS :  0.660752 ---- EVAL LOSS 0.672917\n",
      "STEP:  80  ---- TRAIN LOSS :  0.660771 ---- EVAL LOSS 0.677411\n",
      "STEP:  81  ---- TRAIN LOSS :  0.662192 ---- EVAL LOSS 0.681604\n",
      "STEP:  82  ---- TRAIN LOSS :  0.662872 ---- EVAL LOSS 0.680852\n",
      "STEP:  83  ---- TRAIN LOSS :  0.660983 ---- EVAL LOSS 0.68343\n",
      "STEP:  84  ---- TRAIN LOSS :  0.660527 ---- EVAL LOSS 0.688884\n",
      "STEP:  85  ---- TRAIN LOSS :  0.661247 ---- EVAL LOSS 0.680581\n",
      "STEP:  86  ---- TRAIN LOSS :  0.659228 ---- EVAL LOSS 0.682967\n",
      "STEP:  87  ---- TRAIN LOSS :  0.662418 ---- EVAL LOSS 0.686283\n",
      "STEP:  88  ---- TRAIN LOSS :  0.660335 ---- EVAL LOSS 0.687833\n",
      "STEP:  89  ---- TRAIN LOSS :  0.659954 ---- EVAL LOSS 0.695944\n",
      "STEP:  90  ---- TRAIN LOSS :  0.66178 ---- EVAL LOSS 0.676585\n",
      "STEP:  91  ---- TRAIN LOSS :  0.661316 ---- EVAL LOSS 0.676091\n",
      "STEP:  92  ---- TRAIN LOSS :  0.663206 ---- EVAL LOSS 0.679846\n",
      "STEP:  93  ---- TRAIN LOSS :  0.661539 ---- EVAL LOSS 0.671673\n",
      "STEP:  94  ---- TRAIN LOSS :  0.663189 ---- EVAL LOSS 0.676241\n",
      "STEP:  95  ---- TRAIN LOSS :  0.661718 ---- EVAL LOSS 0.679696\n",
      "STEP:  96  ---- TRAIN LOSS :  0.661882 ---- EVAL LOSS 0.688192\n",
      "STEP:  97  ---- TRAIN LOSS :  0.661883 ---- EVAL LOSS 0.683783\n",
      "STEP:  98  ---- TRAIN LOSS :  0.662105 ---- EVAL LOSS 0.686181\n",
      "STEP:  99  ---- TRAIN LOSS :  0.661243 ---- EVAL LOSS 0.690601\n"
     ]
    }
   ],
   "source": [
    "# setup model\n",
    "model = LaneswapModel(params)\n",
    "if params.settings.cuda:\n",
    "    model.cuda(0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params.learning.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "logger = Logger(params.settings.model_dir + '/checkpoints')\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "j = 0\n",
    "k = 0\n",
    "for step in range(params.learning.max_steps):\n",
    "    mode = 'train'\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    model.anneal_parameters(step)\n",
    "    logger.scalar_summary('train kl weight', model.kl_weight, step)\n",
    "    for (i, (x, y)) in enumerate(train_loader):\n",
    "        if params.settings.cuda:\n",
    "            x['data'] = x['data'].cuda()\n",
    "            x['tl'] = x['tl'].cuda()\n",
    "            x['tl_max'] = x['tl_max'].cuda()\n",
    "#             x['collision'] = x['collision'].cuda()\n",
    "            y = y.cuda()\n",
    "            model.kl_weight = model.kl_weight.cuda()\n",
    "#         pis, mus, sigmas, (loss, ll, kl) = model(x, y, mode)\n",
    "        p_y, (loss, ll, kl) = model(x, y, mode)\n",
    "#         print('ll: ',ll.cpu().data.numpy(), '      kl: ', kl.cpu().data.numpy())\n",
    "        # logging\n",
    "        logger.scalar_summary('train loss', loss, j)\n",
    "        logger.scalar_summary('train log likelihood', ll, j)\n",
    "        logger.scalar_summary('train kl divergence', kl, j)\n",
    "        logger.histo_summary('train time to collision', p_y.cpu().data.numpy(), j)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lossi = loss.cpu().data.numpy()\n",
    "        train_loss.append(lossi)\n",
    "        j += 1\n",
    "    train_loss = np.mean(train_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    # evalulation/validation\n",
    "    mode = 'eval'\n",
    "    model.eval()\n",
    "    eval_loss = []\n",
    "    for (i, (x, y)) in enumerate(eval_loader):\n",
    "        if params.settings.cuda:\n",
    "            x['data'] = x['data'].cuda()\n",
    "            x['tl'] = x['tl'].cuda()\n",
    "            x['tl_max'] = x['tl_max'].cuda()\n",
    "#             x['collision'] = x['collision'].cuda()\n",
    "            y = y.cuda()\n",
    "#         pis, mus, sigmas, (loss, ll, kl) = model(x, y, mode)\n",
    "        p_y, (loss, ll, kl) = model(x, y, mode)\n",
    "        # logging\n",
    "        logger.scalar_summary('eval loss', loss, k)\n",
    "        logger.scalar_summary('eval log likelihood', ll, k)\n",
    "        logger.scalar_summary('eval kl divergence', kl, k)\n",
    "        logger.histo_summary('eval time to collision', p_y.cpu().data.numpy(), k)\n",
    "        lossi = loss.cpu().data.numpy()\n",
    "        eval_loss.append(lossi)\n",
    "        k += 1\n",
    "    eval_loss = np.mean(eval_loss)\n",
    "    eval_losses.append(eval_loss)\n",
    "    \n",
    "    scheduler.step(eval_loss)\n",
    "    print('STEP: ', step, ' ---- TRAIN LOSS : ', train_loss, '---- EVAL LOSS', eval_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(LaneswapDataset(eval_data, eval_tl, eval_col, (data_mean, data_std), scale=params.settings.scale), batch_size = params.learning.batch_size, shuffle=True, **kwargs)\n",
    "# test_loader = torch.utils.data.DataLoader(LaneswapDatasetCol(eval_data, eval_tl), batch_size = len(eval_tl), shuffle=True, **kwargs)\n",
    "mode = 'eval'\n",
    "model.eval()\n",
    "eval_loss = []\n",
    "for (i, (x, y)) in enumerate(test_loader):\n",
    "    if params.settings.cuda:\n",
    "        x['data'] = x['data'].cuda()\n",
    "        x['tl'] = x['tl'].cuda()\n",
    "        x['tl_max'] = x['tl_max'].cuda()\n",
    "#         x['collision'] = x['collision'].cuda()\n",
    "        y = y.cuda()\n",
    "    mu, sigma = model.encoder(x, y, mode)\n",
    "    z = model.reparameterize(mu, sigma)\n",
    "    p_y = model.decoder(z)\n",
    "    p = model(x, y, mode)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4bf70b0a674f7bb6a06b133913eb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.foo>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(i):\n",
    "    log_p_y = torch.log(torch.exp(p_y - torch.logsumexp(p_y, dim=-1, keepdim=True)))\n",
    "    lpy = torch.exp(log_p_y)[i].cpu().detach().numpy()\n",
    "    print(lpy)\n",
    "    plt.bar(range(2), lpy)\n",
    "    plt.bar(y[i].cpu().detach().numpy(), lpy[y[i].cpu().detach().numpy()])\n",
    "    plt.axis([-0.5, 2.5, 0, 1])\n",
    "#     plt.title(y[i].cpu().detach().numpy())\n",
    "    plt.show()\n",
    "interact(foo,i=(0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
