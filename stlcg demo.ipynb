{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A short demo on how to use the stlcg toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember, sequences are reversed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scripts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bbdf10ba0fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mABC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstlcg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstlviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/stlcg/src/stlcg.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Assume inputs are already reversed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scripts'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from src.stlcg import *\n",
    "from src.stlviz import *\n",
    "from src.util import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = np.array([5, 4, 3, 2, 1, 0, 1, 2, 3, 4], dtype=np.float32).reshape([1, 10, 1])\n",
    "w_np = np.array([0, 1, 2, 3, 4, 3, 3, 3, 2, 2], dtype=np.float32).reshape([1, 10, 1])\n",
    "x = torch.tensor(x_np, requires_grad=False)\n",
    "w = torch.tensor(w_np, requires_grad=False)\n",
    "c = torch.tensor(4.0, dtype=torch.float, requires_grad=True)\n",
    "d = torch.tensor(4.0, dtype=torch.float, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GThan = GreaterThan(name=\"x\", c=c)\n",
    "print(GThan)\n",
    "LThan = LessThan(name=\"w\", c=d)\n",
    "print(LThan)\n",
    "Eq = Equal(name=\"x\", c=d)\n",
    "print(Eq)\n",
    "#An = And(subformula1=LThan, subformula2=GThan)\n",
    "An = ~LThan #LThan & GThan\n",
    "print(An)\n",
    "Alw = Always(subformula=An)\n",
    "print(Alw)\n",
    "Ev = Eventually(subformula=An)\n",
    "print(Ev)\n",
    "Unt = Until(subformula1=GThan, subformula2=Always(subformula=LThan))\n",
    "print(Unt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_stl_graph(GThan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_stl_graph(LThan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_stl_graph(Eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_stl_graph(Alw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_stl_graph(Ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_stl_graph(Unt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = make_stl_graph(Unt)\n",
    "save_graph(dot, \"until_graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing grad functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "learning_rate = 0.01\n",
    "def print_learning_progress(var_dict, i, loss):\n",
    "    vals = [i, loss]\n",
    "    string = \"iteration: %i -- loss: %.3f\"\n",
    "    for (k,v) in var_dict.items():\n",
    "        string += \" ---- %s:%.3f\"\n",
    "        vals.append(k)\n",
    "        vals.append(v)\n",
    "    print(string%tuple(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = np.array([5, 4, 3, 2, 1, 0, 1, 2, 3, 4], dtype=np.float32).reshape([1, 10, 1])\n",
    "w_np = np.array([2, 2, 2, 3, 4, 3, 3, 3, 2, 2], dtype=np.float32).reshape([1, 10, 1])\n",
    "x = torch.tensor(x_np, requires_grad=False)\n",
    "w = torch.tensor(w_np, requires_grad=False)\n",
    "c = torch.tensor(6.0, dtype=torch.float, requires_grad=True)\n",
    "d = torch.tensor(1.0, dtype=torch.float, requires_grad=True)\n",
    "W = w + torch.randn([20,10,1], requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GThan = GreaterThan(name=\"w\", c=c)\n",
    "LThan = LessThan(name=\"w\", c=d)\n",
    "\n",
    "Unt = Until(subformula1=LThan, subformula2=Always(subformula=GThan))\n",
    "Th = Then(subformula1=LThan, subformula2=GThan)\n",
    "print(Th)\n",
    "model = Th\n",
    "var_dict = {\"c\":c, \"d\":d}\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(var_dict.values(), lr=learning_rate)\n",
    "scale = 5\n",
    "make_stl_graph(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    learning_rate = 0.01 - 0.01*i/500. + 0.000001\n",
    "    scale = 5 - i/500.\n",
    "    trace1 = model.subformula1(w, scale=scale)\n",
    "    trace2 = model.subformula2(w, scale=scale)\n",
    "    robustness = model.robustness(trace1, trace2, scale=scale)\n",
    "#     print(robustness)\n",
    "    loss = torch.abs(model.robustness(trace1, trace2, scale=scale)).mean()\n",
    "    print_learning_progress(var_dict, i, loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        c -= learning_rate * c.grad\n",
    "        c.grad.zero_()\n",
    "        d -= learning_rate * d.grad\n",
    "        d.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
